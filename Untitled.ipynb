{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [title]\n",
      "Index: []\n",
      "https://www.tripadvisor.in/\n",
      "[]\n",
      "[]\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import selenium\n",
    "import io\n",
    "import requests\n",
    "import bs4\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "from selenium import webdriver\n",
    "import csv\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "from _datetime import datetime\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "options=webdriver.ChromeOptions()\n",
    "options.headless=False\n",
    "prefs={\"profile.default_content_setting_values.notofications\" :2}\n",
    "options.add_experimental_option(\"prefs\",prefs)\n",
    "driver=webdriver.Chrome(\"/Users/maxrojas/Downloads/chromedriver\")\n",
    "driver.maximize_window()\n",
    "time.sleep(2)\n",
    "driver.get(\"https://www.tripadvisor.in/\")\n",
    "#driver.find_element_by_id(\"brand-quick-links-QuickLinkTileItem__link--1k5lE\").click()\n",
    "#driver.find_element_by_id(\"userId\").send_keys(email)\n",
    "#driver.find_element_by_id(\"pwd\").send_keys(pswd)\n",
    "time.sleep(3)\n",
    "\n",
    "#driver.find_element_by_xpath('//*[@id=\"BODY_BLOCK_JQUERY_REFLOW\"]/span/div[2]').click()\n",
    "#time.sleep(2)\n",
    "driver.find_element_by_xpath('//*[@id=\"component_5\"]/div/div/div/span[1]/div/div/div/a').click()\n",
    "driver.find_element_by_xpath('//*[@id=\"BODY_BLOCK_JQUERY_REFLOW\"]//input').send_keys(\"kolkata\",Keys.ENTER)\n",
    "time.sleep(3)\n",
    "total =[]\n",
    "quotes = driver.find_elements_by_class_name(\"listing_title\")\n",
    "for listing_title in quotes:\n",
    "    listing_title = listing_title.find_element_by_class_name('text').text[1:]\n",
    "    new = ((title))\n",
    "    total.append(new)\n",
    "df = pd.DataFrame(total,columns=['title'])\n",
    "df.to_csv('quoted.csv')    \n",
    "print(df)\n",
    "url = driver.current_url\n",
    "print(url)\n",
    "\n",
    "responce=requests.get(url)\n",
    "responce=responce.text\n",
    "data=bs4.BeautifulSoup(responce,'lxml')\n",
    "\n",
    "read1=data.select(\".listing_title\")\n",
    "read2=data.select(\".price __resizeWatch \")\n",
    "name=[]\n",
    "for i in range(len(read1)):\n",
    "    x=read1[i].text\n",
    "    name.append(x)\n",
    "print(name)\n",
    "price=[]\n",
    "for i in read2:\n",
    "    x=i.text\n",
    "    x=x.lstrip()\n",
    "    x=x.split(\" \")\n",
    "\n",
    "    if (len(x)>1):\n",
    "        price.append(str(x[1]))\n",
    "    else:\n",
    "        price.append(str(x[0]))\n",
    "print(price)\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "df=pd.DataFrame(name, price)\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.tripadvisor.com/Attraction_Review-g189852-d208277-Reviews-Vasa_Museum-Stockholm.html\n",
      "[17986]\n",
      "                          Title  \\\n",
      "0   Great Museum but rude staff   \n",
      "1   Great Museum but rude staff   \n",
      "2   Great Museum but rude staff   \n",
      "3   Great Museum but rude staff   \n",
      "4   Great Museum but rude staff   \n",
      "5   Great Museum but rude staff   \n",
      "6   Great Museum but rude staff   \n",
      "7   Great Museum but rude staff   \n",
      "8   Great Museum but rude staff   \n",
      "9   Great Museum but rude staff   \n",
      "10  Great Museum but rude staff   \n",
      "11  Great Museum but rude staff   \n",
      "12                 Amazing ship   \n",
      "13                     Amazing!   \n",
      "\n",
      "                                               Review Rating Likes  \\\n",
      "0   To have a museum built around a ship is remark...            2   \n",
      "1   To have a museum built around a ship is remark...            2   \n",
      "2   To have a museum built around a ship is remark...            2   \n",
      "3   To have a museum built around a ship is remark...            2   \n",
      "4   To have a museum built around a ship is remark...            2   \n",
      "5   To have a museum built around a ship is remark...            2   \n",
      "6   To have a museum built around a ship is remark...            2   \n",
      "7   To have a museum built around a ship is remark...            2   \n",
      "8   To have a museum built around a ship is remark...            2   \n",
      "9   To have a museum built around a ship is remark...            2   \n",
      "10  To have a museum built around a ship is remark...            2   \n",
      "11  To have a museum built around a ship is remark...            2   \n",
      "12  This is a amazing opportunity to visit to see ...           19   \n",
      "13  Sent with the family and all of us found it ve...           13   \n",
      "\n",
      "                           Experience           Location  \n",
      "0      Date of experience: March 2020  Stockholm, Sweden  \n",
      "1      Date of experience: March 2020  Stockholm, Sweden  \n",
      "2      Date of experience: March 2020  Stockholm, Sweden  \n",
      "3      Date of experience: March 2020  Stockholm, Sweden  \n",
      "4      Date of experience: March 2020  Stockholm, Sweden  \n",
      "5      Date of experience: March 2020  Stockholm, Sweden  \n",
      "6      Date of experience: March 2020  Stockholm, Sweden  \n",
      "7      Date of experience: March 2020  Stockholm, Sweden  \n",
      "8      Date of experience: March 2020  Stockholm, Sweden  \n",
      "9      Date of experience: March 2020  Stockholm, Sweden  \n",
      "10     Date of experience: March 2020  Stockholm, Sweden  \n",
      "11     Date of experience: March 2020  Stockholm, Sweden  \n",
      "12      Date of experience: June 2019  Stockholm, Sweden  \n",
      "13  Date of experience: February 2020  Stockholm, Sweden  \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-164-97a5cd93fe18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m          \u001b[0mReview\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparagraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'location-review-review-list-parts-ExpandableReview__reviewText--gOmRC'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m          \u001b[0mRating\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparagraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ui_bubble_rating bubble_50'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m          \u001b[0mLikes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparagraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'social-member-MemberHeaderStats__bold--3z3qh'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m          \u001b[0mExperience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparagraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'location-review-review-list-parts-EventDate__event_date--1epHa'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m          \u001b[0mLocation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparagraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'default social-member-common-MemberHometown__hometown--3kM9S small'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "final_list=[]#final list to be the df\n",
    "import json\n",
    "import requests\n",
    "final_list=[]\n",
    "import numpy as np\n",
    "url = 'https://www.tripadvisor.com/Attraction_Review-g189852-d208277-Reviews-or'\n",
    "url2 = '-Vasa_Museum-Stockholm.html#REVIEWS'\n",
    "urlFinal = url +url2\n",
    "#making a request to get the number of reviews\n",
    "r=requests.get(urlFinal)\n",
    "soup = BeautifulSoup(r.text, 'lxml')\n",
    "review_count_h2=soup.find(class_=\"location-review-review-list-parts-LanguageFilter__paren_count--2vk3f\").text \n",
    "review_count_h2=review_count_h2.replace(\",\", \".\").replace('(', '').replace(')', '')\n",
    "nums = np.array(review_count_h2.split(), dtype=np.float)\n",
    "\n",
    "\n",
    "#there are 5 reviews per page so pages can be calculated as\n",
    "pages=int(nums*1000/5)+1\n",
    "\n",
    "#change range to 1 to pages+1\n",
    "for pg in range(5, 10, 5):\n",
    "  pg = url + str(pg) + url2\n",
    "  r=requests.get(pg)\n",
    "  soup = BeautifulSoup(r.text, 'lxml')\n",
    "  for paragraph in soup.find_all():\n",
    "     try:\n",
    "         Title=paragraph.find(class_='location-review-review-list-parts-ReviewTitle__reviewTitleText--2tFRT').text.strip()\n",
    "         Review=paragraph.find(class_='location-review-review-list-parts-ExpandableReview__reviewText--gOmRC').text.strip()\n",
    "         Rating=paragraph.find(class_='ui_bubble_rating bubble_50')\n",
    "         Likes=paragraph.fin(class_='social-member-MemberHeaderStats__bold--3z3qh').text.strip()\n",
    "         Experience=paragraph.find(class_='location-review-review-list-parts-EventDate__event_date--1epHa')\n",
    "         Location=paragraph.find(class_='default social-member-common-MemberHometown__hometown--3kM9S small')\n",
    "         final_list.append([Title,Review,Rating,Likes,Experience,Location])\n",
    "     except AttributeError:\n",
    "        pass\n",
    "df = pd.DataFrame(final_list,columns=['Title','Review','Rating','Likes','Location'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidSelectorException",
     "evalue": "Message: invalid selector: Unable to locate an element with the xpath expression //*[@id=\"component_18\"]/ because of the following error:\nSyntaxError: Failed to execute 'evaluate' on 'Document': The string '//*[@id=\"component_18\"]/' is not a valid XPath expression.\n  (Session info: chrome=80.0.3987.132)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidSelectorException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-a1a060f27a10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element_by_xpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'//*[@id=\"component_18\"]/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mfind_element_by_xpath\u001b[0;34m(self, xpath)\u001b[0m\n\u001b[1;32m    393\u001b[0m         \"\"\"\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mfind_element\u001b[0;34m(self, by, value)\u001b[0m\n\u001b[1;32m    977\u001b[0m             \u001b[0;34m'using'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m             'value': value})['value']\n\u001b[0m\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidSelectorException\u001b[0m: Message: invalid selector: Unable to locate an element with the xpath expression //*[@id=\"component_18\"]/ because of the following error:\nSyntaxError: Failed to execute 'evaluate' on 'Document': The string '//*[@id=\"component_18\"]/' is not a valid XPath expression.\n  (Session info: chrome=80.0.3987.132)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidSelectorException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-a1a060f27a10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element_by_xpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'//*[@id=\"component_18\"]/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mfind_element_by_xpath\u001b[0;34m(self, xpath)\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element_by_xpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'//div/td[1]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \"\"\"\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_elements_by_xpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mfind_element\u001b[0;34m(self, by, value)\u001b[0m\n\u001b[1;32m    976\u001b[0m         return self.execute(Command.FIND_ELEMENT, {\n\u001b[1;32m    977\u001b[0m             \u001b[0;34m'using'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m             'value': value})['value']\n\u001b[0m\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_elements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[1;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alert'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidSelectorException\u001b[0m: Message: invalid selector: Unable to locate an element with the xpath expression //*[@id=\"component_18\"]/ because of the following error:\nSyntaxError: Failed to execute 'evaluate' on 'Document': The string '//*[@id=\"component_18\"]/' is not a valid XPath expression.\n  (Session info: chrome=80.0.3987.132)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "import datetime\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "#Common\n",
    "now = datetime.datetime.now()\n",
    "driver = webdriver.Chrome('/Users/maxrojas/Downloads/chromedriver')\n",
    "italia = \"https://www.tripadvisor.com/Attraction_Review-g189852-d208277-Reviews-Vasa_Museum-Stockholm.html\"\n",
    "driver.get(italia)\n",
    "\n",
    "place = 'Ex_Stabilimento_Florio_delle_Tonnare_di_Favignana'\n",
    "lang = 'it'\n",
    "\n",
    "\n",
    "def check_exists_by_xpath(xpath):\n",
    "    try:\n",
    "        driver.find_element_by_xpath(xpath)\n",
    "    except NoSuchElementException:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0, 2):\n",
    "    try:\n",
    "        if (check_exists_by_xpath(\"//span[@class='taLnk ulBlueLinks']\")):\n",
    "            driver.find_element_by_xpath(\"//span[@class='taLnk ulBlueLinks']\").click()\n",
    "            time.sleep(5)\n",
    "        container = driver.find_elements_by_xpath(\"//div[@class='review-container']\")\n",
    "\n",
    "        num_page_items = len(container)\n",
    "        for j in range(num_page_items):\n",
    "\n",
    "            csvFile = open(r'Italia_en.csv', 'a')\n",
    "            csvWriter = csv.writer(csvFile)\n",
    "\n",
    "            time.sleep(10)\n",
    "            rating_a = container[j].find_element_by_xpath(\n",
    "                \".//span[contains(@class, 'ui_bubble_rating bubble_')]\").get_attribute(\"class\")\n",
    "            rating_b = rating_a.split(\"_\")\n",
    "            rating = rating_b[3]\n",
    "\n",
    "            review = container[j].find_element_by_xpath(\".//p[@class='partial_entry']\").text.replace(\"\\n\", \"\")\n",
    "            title = container[j].find_element_by_class_name('quote').find_element_by_tag_name(\n",
    "                'a').find_element_by_class_name('noQuotes').text\n",
    "\n",
    "            print(review)\n",
    "            rating_date = container[j].find_element_by_class_name('ratingDate').get_attribute('title')\n",
    "            print(rating, review, title, \"--\", sep='\\n')\n",
    "            link_list = []\n",
    "            for link in container[j].find_elements_by_tag_name('a'):\n",
    "                link_previous = (link.get_attribute('href'))\n",
    "                link_list.append(link_previous)\n",
    "\n",
    "            print(link_list[1], \"--\", sep='\\n')\n",
    "\n",
    "            csvWriter.writerow([place, rating, title, review, rating_date, link_list[1], now, lang])\n",
    "\n",
    "        driver.find_element_by_xpath('//*[@id=\"component_18\"]/').click()\n",
    "\n",
    "        time.sleep(5)\n",
    "\n",
    "    except:\n",
    "\n",
    "        driver.find_element_by_xpath('//*[@id=\"component_18\"]/').click()\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchElementException",
     "evalue": "Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//*[@id=\"component_14\"]/div[3]/div/div[8]/div/div/a[6]\"}\n  (Session info: chrome=80.0.3987.132)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-154-a168701c7bc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lxml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mreview_count_h2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element_by_xpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'//*[@id=\"component_14\"]/div[3]/div/div[8]/div/div/a[6]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mreview_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview_count_h2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#there are 20 reviews per page so pages can be calculated as\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mfind_element_by_xpath\u001b[0;34m(self, xpath)\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element_by_xpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'//div/td[1]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \"\"\"\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_elements_by_xpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mfind_element\u001b[0;34m(self, by, value)\u001b[0m\n\u001b[1;32m    976\u001b[0m         return self.execute(Command.FIND_ELEMENT, {\n\u001b[1;32m    977\u001b[0m             \u001b[0;34m'using'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m             'value': value})['value']\n\u001b[0m\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_elements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[1;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alert'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//*[@id=\"component_14\"]/div[3]/div/div[8]/div/div/a[6]\"}\n  (Session info: chrome=80.0.3987.132)\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import math\n",
    "import pandas as pd\n",
    "final_list=[]#final list to be the df\n",
    "import json\n",
    "import requests\n",
    "final_list=[]\n",
    "url = 'https://uk.trustpilot.com/review/thread.com'\n",
    "#making a request to get the number of reviews\n",
    "r=requests.get(url)\n",
    "soup = BeautifulSoup(r.text, 'lxml')\n",
    "review_count_h2=driver.find_element_by_xpath('//*[@id=\"component_14\"]/div[3]/div/div[8]/div/div/a[6]').text.strip()\n",
    "review_count=int(review_count_h2.strip().split(' ')[0].strip())\n",
    "#there are 20 reviews per page so pages can be calculated as\n",
    "pages=int(math.ceil(review_count/20))\n",
    "#change range to 1 to pages+1\n",
    "for pg in range(1, pages+1):\n",
    "  pg = url + '?page=' + str(pg)\n",
    "  r=requests.get(pg)\n",
    "  soup = BeautifulSoup(r.text, 'lxml')\n",
    "  for paragraph in soup.find_all(class_='location-review-review-list-parts-SingleReview__mainCol--1hApa'):\n",
    "     try:\n",
    "         title=paragraph.find('q',class_='location-review-review-list-parts-ExpandableReview__reviewText--gOmRC').text.strip()\n",
    "         content=paragraph.find('p',class_='review-content__text').text.strip()\n",
    "         datedata= json.loads(paragraph.find('div',class_='review-content-header__dates').text)\n",
    "         date=datedata['publishedDate'].split('T')[0]\n",
    "         rating_class=paragraph.find('div',class_='star-rating')['class']\n",
    "         rating=rating_class[1].split('-')[-1]\n",
    "         final_list.append([title])\n",
    "     except AttributeError:\n",
    "        pass\n",
    "df = pd.DataFrame(final_list,columns=['Title','Content','Date','Rating'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_count=float(review_count_h2.strip(\"\\r\\n\\t '\").strip('\"').split(' ')[0].strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Title  \\\n",
      "0                                   Very well done.   \n",
      "1                                   Very well done.   \n",
      "2                                   Very well done.   \n",
      "3                                   Very well done.   \n",
      "4                                   Very well done.   \n",
      "5                                   Very well done.   \n",
      "6                                   Very well done.   \n",
      "7                                   Very well done.   \n",
      "8                                   Very well done.   \n",
      "9                                   Very well done.   \n",
      "10                                  Very well done.   \n",
      "11                                  Very well done.   \n",
      "12                                  Very well done.   \n",
      "13                                  Very well done.   \n",
      "14                                Very interesting!   \n",
      "15                                Very interesting!   \n",
      "16  À must see because it's one of a kind and more.   \n",
      "17  À must see because it's one of a kind and more.   \n",
      "18                               ABSOLUTELY AMAZING   \n",
      "19                               ABSOLUTELY AMAZING   \n",
      "20                                     Great museum   \n",
      "21                                     Great museum   \n",
      "\n",
      "                                               Review Rating  \\\n",
      "0   What has been done with the restoration and ex...   [[]]   \n",
      "1   What has been done with the restoration and ex...   [[]]   \n",
      "2   What has been done with the restoration and ex...   [[]]   \n",
      "3   What has been done with the restoration and ex...   [[]]   \n",
      "4   What has been done with the restoration and ex...   [[]]   \n",
      "5   What has been done with the restoration and ex...   [[]]   \n",
      "6   What has been done with the restoration and ex...   [[]]   \n",
      "7   What has been done with the restoration and ex...   [[]]   \n",
      "8   What has been done with the restoration and ex...   [[]]   \n",
      "9   What has been done with the restoration and ex...   [[]]   \n",
      "10  What has been done with the restoration and ex...   [[]]   \n",
      "11  What has been done with the restoration and ex...   [[]]   \n",
      "12  What has been done with the restoration and ex...   [[]]   \n",
      "13  What has been done with the restoration and ex...   [[]]   \n",
      "14  It was truly amazing to see this ship so well ...   [[]]   \n",
      "15  It was truly amazing to see this ship so well ...   [[]]   \n",
      "16  Extremely interesting and well done displayes ...   [[]]   \n",
      "17  Extremely interesting and well done displayes ...   [[]]   \n",
      "18  Great museum with the displays really well set...   [[]]   \n",
      "19  Great museum with the displays really well set...   [[]]   \n",
      "20  One of the best museums in Stockholm. Be supri...   [[]]   \n",
      "21  One of the best museums in Stockholm. Be supri...   [[]]   \n",
      "\n",
      "                                   Likes                 Location  \n",
      "0   [[Date of experience:],  March 2020]  [[], Sydney, Australia]  \n",
      "1   [[Date of experience:],  March 2020]  [[], Sydney, Australia]  \n",
      "2   [[Date of experience:],  March 2020]  [[], Sydney, Australia]  \n",
      "3   [[Date of experience:],  March 2020]  [[], Sydney, Australia]  \n",
      "4   [[Date of experience:],  March 2020]  [[], Sydney, Australia]  \n",
      "5   [[Date of experience:],  March 2020]  [[], Sydney, Australia]  \n",
      "6   [[Date of experience:],  March 2020]  [[], Sydney, Australia]  \n",
      "7   [[Date of experience:],  March 2020]  [[], Sydney, Australia]  \n",
      "8   [[Date of experience:],  March 2020]  [[], Sydney, Australia]  \n",
      "9   [[Date of experience:],  March 2020]  [[], Sydney, Australia]  \n",
      "10  [[Date of experience:],  March 2020]  [[], Sydney, Australia]  \n",
      "11  [[Date of experience:],  March 2020]  [[], Sydney, Australia]  \n",
      "12  [[Date of experience:],  March 2020]  [[], Sydney, Australia]  \n",
      "13  [[Date of experience:],  March 2020]                     None  \n",
      "14  [[Date of experience:],  April 2019]       [[], Oslo, Norway]  \n",
      "15  [[Date of experience:],  April 2019]                     None  \n",
      "16  [[Date of experience:],  March 2020]  [[], Stockholm, Sweden]  \n",
      "17  [[Date of experience:],  March 2020]                     None  \n",
      "18  [[Date of experience:],  March 2020]                     None  \n",
      "19  [[Date of experience:],  March 2020]                     None  \n",
      "20  [[Date of experience:],  March 2020]                     None  \n",
      "21  [[Date of experience:],  March 2020]                     None  \n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import math\n",
    "import pandas as pd\n",
    "final_list=[]#final list to be the df\n",
    "import json\n",
    "import requests\n",
    "final_list=[]\n",
    "import numpy as np\n",
    "url = 'https://www.tripadvisor.com/Attraction_Review-g189852-d208277-Reviews-or'\n",
    "url2 = '-Vasa_Museum-Stockholm.html#REVIEWS'\n",
    "urlFinal = url +url2\n",
    "#making a request to get the number of reviews\n",
    "r=requests.get(urlFinal)\n",
    "soup = BeautifulSoup(r.text, 'lxml')\n",
    "review_count_h2=soup.find(class_=\"location-review-review-list-parts-LanguageFilter__paren_count--2vk3f\").text.strip() \n",
    "review_count_h2=review_count_h2.replace(\",\", \".\").replace('(', '').replace(')', '')\n",
    "nums = np.array(review_count_h2.split(), dtype=np.float)\n",
    "\n",
    "\n",
    "#there are 5 reviews per page so pages can be calculated as\n",
    "pages=int(nums*1000/5)+1\n",
    "\n",
    "#change range to 1 to pages+1\n",
    "for pg in range(5, 10, 5):\n",
    "  pg = url + str(pg) + url2\n",
    "  r=requests.get(pg)\n",
    "  soup = BeautifulSoup(r.text, 'lxml')\n",
    "  for paragraph in soup.find_all():\n",
    "     try:\n",
    "         Title=paragraph.find(class_='location-review-review-list-parts-ReviewTitle__reviewTitleText--2tFRT').text.strip()\n",
    "         Review=paragraph.find(class_='cPQsENeY').text.strip()\n",
    "         Rating=paragraph.find(class_='location-review-review-list-parts-RatingLine__bubbles--GcJvM')\n",
    "         Likes=paragraph.find(class_='location-review-review-list-parts-EventDate__event_date--1epHa')\n",
    "         Location=paragraph.find(class_='default social-member-common-MemberHometown__hometown--3kM9S small')\n",
    "         final_list.append([Title,Review,Rating,Likes,Location])\n",
    "     except AttributeError:\n",
    "        pass\n",
    "df = pd.DataFrame(final_list,columns=['Title','Review','Rating','Likes','Location'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-171-fdc22e5173e0>, line 30)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-171-fdc22e5173e0>\"\u001b[0;36m, line \u001b[0;32m30\u001b[0m\n\u001b[0;31m    container = driver.find_elements_by_xpath(\".//*[@id=\"taplc_resp_attraction_stacking_2col_ar_responsive_0\"]/div/div[2]\")\u001b[0m\n\u001b[0m                                                                                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "import datetime\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "#Common\n",
    "now = datetime.datetime.now()\n",
    "driver = webdriver.Chrome('/Users/maxrojas/Downloads/chromedriver')\n",
    "italia = \"https://www.tripadvisor.it/Attraction_Review-g657290-d2213040-Reviews-Ex_Stabilimento_Florio_delle_Tonnare_di_Favignana_e_Formica-Isola_di_Favig.html\"\n",
    "driver.get(italia)\n",
    "\n",
    "place = 'Ex_Stabilimento_Florio_delle_Tonnare_di_Favignana'\n",
    "lang = 'it'\n",
    "\n",
    "\n",
    "def check_exists_by_xpath(xpath):\n",
    "    try:\n",
    "        driver.find_element_by_xpath(xpath)\n",
    "    except NoSuchElementException:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0, 2):\n",
    "    try:\n",
    "        container = driver.find_elements_by_xpath(\".//*[@id=\"taplc_resp_attraction_stacking_2col_ar_responsive_0\"]/div/div[2]\")\n",
    "\n",
    "        num_page_items = len(container)\n",
    "        for j in range(num_page_items):\n",
    "\n",
    "            csvFile = open(r'Italia_en.csv', 'a')\n",
    "            csvWriter = csv.writer(csvFile)\n",
    "\n",
    "            time.sleep(10)\n",
    "            rating_a = container[j].find_element_by_xpath(\n",
    "                \".//span[contains(@class, 'ui_bubble_rating bubble_')]\").get_attribute(\"class\")\n",
    "            rating_b = rating_a.split(\"_\")\n",
    "            rating = rating_b[3]\n",
    "\n",
    "            review = container[j].find_element_by_xpath(\".//p[@class='partial_entry']\").text.replace(\"\\n\", \"\")\n",
    "            title = container[j].find_element_by_class_name('quote').find_element_by_tag_name(\n",
    "                'a').find_element_by_class_name('noQuotes').text\n",
    "\n",
    "            print(review)\n",
    "            rating_date = container[j].find_element_by_class_name('ratingDate').get_attribute('title')\n",
    "            print(rating, review, title, \"--\", sep='\\n')\n",
    "            link_list = []\n",
    "            for link in container[j].find_elements_by_tag_name('a'):\n",
    "                link_previous = (link.get_attribute('href'))\n",
    "                link_list.append(link_previous)\n",
    "\n",
    "            print(link_list[1], \"--\", sep='\\n')\n",
    "\n",
    "            csvWriter.writerow([place, rating, title, review, rating_date, link_list[1], now, lang])\n",
    "\n",
    "        driver.find_element_by_xpath('//a[@class=\"nav next taLnk ui_button primary\"]').click()\n",
    "\n",
    "        time.sleep(5)\n",
    "\n",
    "    except:\n",
    "\n",
    "        driver.find_element_by_xpath('//a[@class=\"nav next taLnk ui_button primary\"]').click()\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[parse] url: https://www.tripadvisor.com/Restaurant_Review-g187823-d2101904-Reviews-Eataly_Genova-Genoa_Italian_Riviera_Liguria.html?filterLang=ALL\n",
      "[parse] num_reviews ALL: 4933\n",
      "[parse] url_template: https://www.tripadvisor.com/Restaurant_Review-g187823-d2101904-Reviews-Eataly_Genova-Genoa_Italian_Riviera_Liguria-or{}.html?filterLang=ALL\n",
      "[parse_reviews] url: https://www.tripadvisor.com/Restaurant_Review-g187823-d2101904-Reviews-Eataly_Genova-Genoa_Italian_Riviera_Liguria-or0.html?filterLang=ALL\n",
      "No reviews\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import webbrowser\n",
    "import io\n",
    "def display(content, filename='output.html'):\n",
    "    with open(filename, 'wb') as f:\n",
    "         f.write(content)\n",
    "         webbrowser.open(filename)\n",
    "\n",
    "reviews_id =[]\n",
    "def get_soup(session, url, show=False):\n",
    "    r = session.get(url)\n",
    "    if show:\n",
    "        display(r.content, 'temp.html')\n",
    "    if r.status_code != 200: # not OK\n",
    "        print('[get_soup] status code:', r.status_code)\n",
    "    else:\n",
    "        return BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "\n",
    "def post_soup(session, url, params, show=False):\n",
    "    '''Read HTML from server and convert to Soup'''\n",
    "\n",
    "    r = session.post(url, data=params)\n",
    "\n",
    "    if show:\n",
    "        display(r.content, 'temp.html')\n",
    "\n",
    "    if r.status_code != 200: # not OK\n",
    "        print('[post_soup] status code:', r.status_code)\n",
    "    else:\n",
    "        return BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "def scrape(url, lang='ALL'):\n",
    "\n",
    "    # create session to keep all cookies (etc.) between requests\n",
    "    session = requests.Session()\n",
    "\n",
    "    session.headers.update({\n",
    "        'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:57.0) Gecko/20100101 Firefox/57.0',\n",
    "    })\n",
    "\n",
    "\n",
    "    items = parse(session, url + '?filterLang=' + lang)\n",
    "\n",
    "    return items\n",
    "\n",
    "def parse(session, url):\n",
    "    '''Get number of reviews and start getting subpages with reviews'''\n",
    "\n",
    "    print('[parse] url:', url)\n",
    "\n",
    "    soup = get_soup(session, url)\n",
    "\n",
    "    if not soup:\n",
    "        print('[parse] no soup:', url)\n",
    "        return\n",
    "\n",
    "    num_reviews = soup.find('span', class_='reviews_header_count').text # get text\n",
    "    num_reviews = num_reviews[1:-1] \n",
    "    num_reviews = num_reviews.replace(',', '')\n",
    "    num_reviews = int(num_reviews) # convert text into integer\n",
    "    print('[parse] num_reviews ALL:', num_reviews)\n",
    "\n",
    "    url_template = url.replace('.html', '-or{}.html')\n",
    "    print('[parse] url_template:', url_template)\n",
    "\n",
    "    items = []\n",
    "\n",
    "    offset = 0\n",
    "\n",
    "    while(True):\n",
    "        subpage_url = url_template.format(offset)\n",
    "\n",
    "        subpage_items = parse_reviews(session, subpage_url)\n",
    "        if not subpage_items:\n",
    "            break\n",
    "\n",
    "        items += subpage_items\n",
    "\n",
    "        if len(subpage_items) < 5:\n",
    "            break\n",
    "\n",
    "        offset += 5\n",
    "\n",
    "    return items\n",
    "\n",
    "def get_reviews_ids(soup):\n",
    "\n",
    "    items = soup.find_all('div', attrs={'data-reviewid': True})\n",
    "\n",
    "    if items:\n",
    "        reviews_ids = [x.attrs['data-reviewid'] for x in items][::2]\n",
    "        print('[get_reviews_ids] data-reviewid:', reviews_ids)\n",
    "        return reviews_ids\n",
    "\n",
    "def get_more(session, reviews_ids):\n",
    "\n",
    "    url = 'https://www.tripadvisor.com/OverlayWidgetAjax?Mode=EXPANDED_HOTEL_REVIEWS_RESP&metaReferer=Hotel_Review'\n",
    "\n",
    "    payload = {\n",
    "        'reviews': ','.join(reviews_ids), # ie. \"577882734,577547902,577300887\",\n",
    "        #'contextChoice': 'DETAIL_HR', # ???\n",
    "        'widgetChoice': 'EXPANDED_HOTEL_REVIEW_HSX', # ???\n",
    "        'haveJses': 'earlyRequireDefine,amdearly,global_error,long_lived_global,apg-Hotel_Review,apg-Hotel_Review-in,bootstrap,desktop-rooms-guests-dust-en_US,responsive-calendar-templates-dust-en_US,taevents',\n",
    "        'haveCsses': 'apg-Hotel_Review-in',\n",
    "        'Action': 'install',\n",
    "    }\n",
    "\n",
    "    soup = post_soup(session, url, payload)\n",
    "\n",
    "    return soup\n",
    "\n",
    "def parse_reviews(session, url):\n",
    "    '''Get all reviews from one page'''\n",
    "\n",
    "    print('[parse_reviews] url:', url)\n",
    "\n",
    "    soup =  get_soup(session, url)\n",
    "    reviews_ids =[]\n",
    "    if not soup:\n",
    "        print('[parse_reviews] no soup:', url)\n",
    "        return\n",
    "        reviews_ids =[]\n",
    "        reviews_ids = get_reviews_ids(soup)\n",
    "        hotel_name = soup.find('h1', class_='').text\n",
    "\n",
    "        \n",
    "    if not reviews_ids:\n",
    "        return\n",
    "\n",
    "    soup = get_more(session, reviews_ids)\n",
    "\n",
    "    if not soup:\n",
    "        print('[parse_reviews] no soup:', url)\n",
    "        return\n",
    "\n",
    "    items = []\n",
    "\n",
    "    for idx, review in enumerate(soup.find_all('div', class_='reviewSelector')):\n",
    "\n",
    "        badgets = review.find_all('span', class_='badgetext')\n",
    "        if len(badgets) > 0:\n",
    "            contributions = badgets[0].text\n",
    "        else:\n",
    "            contributions = '0'\n",
    "\n",
    "        if len(badgets) > 1:\n",
    "            helpful_vote = badgets[1].text\n",
    "        else:\n",
    "            helpful_vote = '0'\n",
    "        user_loc = review.select_one('div.userLoc strong')\n",
    "        if user_loc:\n",
    "            user_loc = user_loc.text\n",
    "        else:\n",
    "            user_loc = ''\n",
    "\n",
    "        bubble_rating = review.select_one('span.ui_bubble_rating')['class']\n",
    "        bubble_rating = bubble_rating[1].split('_')[-1]\n",
    "\n",
    "        item = {\n",
    "            'review_body': review.find('p', class_='partial_entry').text,\n",
    "            'review_date': review.find('span', class_='ratingDate')['title'], # 'ratingDate' instead of 'relativeDate'\n",
    "       }\n",
    "\n",
    "        items.append(item)\n",
    "        print('\\n--- review ---\\n')\n",
    "        for key,val in item.items():\n",
    "            print(' ', key, ':', val)\n",
    "\n",
    "    print()\n",
    "\n",
    "    return items\n",
    "\n",
    "def write_in_csv(items, filename='results.csv',\n",
    "                  headers=['hotel name', 'review title', 'review body',\n",
    "                           'review date', 'contributions', 'helpful vote',\n",
    "                           'user name' , 'user location', 'rating'],\n",
    "              mode='w'):\n",
    "\n",
    "    print('--- CSV ---')\n",
    "\n",
    "    with io.open(filename, mode, encoding=\"utf-8\") as csvfile:\n",
    "        csv_file = csv.DictWriter(csvfile, headers)\n",
    "\n",
    "        if mode == 'w':\n",
    "            csv_file.writeheader()\n",
    "\n",
    "        csv_file.writerows(items)\n",
    "\n",
    "\n",
    "DB_COLUMN   = 'review_body'\n",
    "DB_COLUMN1 = 'review_date'\n",
    "start_urls = [\n",
    "    'https://www.tripadvisor.com/Restaurant_Review-g187823-d2101904-Reviews-Eataly_Genova-Genoa_Italian_Riviera_Liguria.html',\n",
    "]\n",
    "headers = [ \n",
    "    DB_COLUMN, \n",
    "    DB_COLUMN1, \n",
    "]\n",
    "\n",
    "\n",
    "for url in start_urls:\n",
    "\n",
    "    # get all reviews for 'url' and 'lang'\n",
    "    items = scrape(url)\n",
    "\n",
    "    if not items:\n",
    "        print('No reviews')\n",
    "    else:\n",
    "        # write in CSV\n",
    "        filename = url.split('Reviews-')[1][:-5]\n",
    "        print('filename:', filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-190-e1e437f752a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msoup3\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview_pages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'lxml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msoup3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.hotels-hotel-review-about-with-photos-Reviews__subratings--3DGjN [class^=\"ui_bubble_rating bubble_\"]'\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m#.hotels-hotel-review-about-with-photos-Reviews__subratings--3DGjN span\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mValue_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mValue_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mstars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\d'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValue_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "review_pages=requests.get(\"https://www.tripadvisor.com/Hotel_Review-g60745-d94367-Reviews-Harborside_Inn-Boston_Massachusetts.html\")  \n",
    "soup3=BeautifulSoup(review_pages.content,'lxml')   \n",
    "values=soup3.select('.hotels-hotel-review-about-with-photos-Reviews__subratings--3DGjN [class^=\"ui_bubble_rating bubble_\"]')    #.hotels-hotel-review-about-with-photos-Reviews__subratings--3DGjN span\n",
    "Value_1 = values[-1]\n",
    "print(Value_1['class'][1])\n",
    "stars = re.search(r'\\d', Value_1['class'][1]).group(0)\n",
    "print(stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.tripadvisor.com/Attraction_Review-g189852-d208277-Reviews-Vasa_Museum-Stockholm.html\n",
      "[17994]\n",
      "--Return--\n",
      "> <ipython-input-193-c3ce699d9d93>(42)<module>()->None\n",
      "-> import pdb; pdb.set_trace()\n"
     ]
    }
   ],
   "source": [
    "import selenium\n",
    "import io\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "from selenium import webdriver\n",
    "import csv\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "import pandas as pd\n",
    "final_list=[]#final list to be the df\n",
    "import json\n",
    "import numpy as np\n",
    "from _datetime import datetime\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "options=webdriver.ChromeOptions()\n",
    "options.headless=False\n",
    "prefs={\"profile.default_content_setting_values.notifications\" :2}\n",
    "options.add_experimental_option(\"prefs\",prefs)\n",
    "driver=webdriver.Chrome(\"/Users/maxrojas/Downloads/chromedriver\")\n",
    "driver.maximize_window()\n",
    "time.sleep(2)\n",
    "driver.get(\"https://www.tripadvisor.com/Attraction_Review-g189852-d208277-Reviews-Vasa_Museum-Stockholm.html\")\n",
    "#driver.find_element_by_id(\"brand-quick-links-QuickLinkTileItem__link--1k5lE\").click()\n",
    "#driver.find_element_by_id(\"userId\").send_keys(email)\n",
    "#driver.find_element_by_id(\"pwd\").send_keys(pswd)\n",
    "\n",
    "total =[]\n",
    "url = driver.current_url\n",
    "print(url)\n",
    "\n",
    "r=requests.get(url)\n",
    "soup = BeautifulSoup(r.text, 'lxml')\n",
    "review_count_h2=soup.find(class_=\"location-review-review-list-parts-LanguageFilter__paren_count--2vk3f\").text.strip() \n",
    "review_count_h2=review_count_h2.replace(\",\", \"\").replace('(', '').replace(')', '')\n",
    "nums = np.array(review_count_h2.split(), dtype=np.int) \n",
    "print(nums)\n",
    "type(nums)\n",
    "#change range to 1 to pages+1\n",
    "import pdb; pdb.set_trace()\n",
    "for pg in range(0, 1):\n",
    "  soup = BeautifulSoup(r.text, 'lxml')\n",
    "  time.sleep(10)\n",
    "\n",
    "#driver.find_element_by_xpath('//*[@id=\"BODY_BLOCK_JQUERY_REFLOW\"]/span/div[2]').click()\n",
    "#time.sleep(4)\n",
    "  driver.find_element_by_class_name('location-review-review-list-parts-ExpandableReview__cta--2mR2g').click()\n",
    "  time.sleep(10)\n",
    "  for paragraph in soup.find_all():\n",
    "     try:\n",
    "         Title=paragraph.find(class_='location-review-review-list-parts-ReviewTitle__reviewTitleText--2tFRT').text.strip()\n",
    "         Review=paragraph.find(class_='location-review-review-list-parts-ExpandableReview__reviewText--gOmRC').text.strip()\n",
    "         Rating=paragraph.find(class_='location-review-review-list-parts-RatingLine__bubbles--GcJvM').text.strip()\n",
    "         Likes=paragraph.find(class_='social-member-MemberHeaderStats__bold--3z3qh').text.strip()\n",
    "         Experience=paragraph.find(class_='location-review-review-list-parts-EventDate__event_date--1epHa').text.strip()\n",
    "         Location=paragraph.find(class_='default social-member-common-MemberHometown__hometown--3kM9S small').text.strip()\n",
    "         final_list.append([Title,Review,Rating,Likes,Experience,Location])\n",
    "     except AttributeError:\n",
    "        pass\n",
    "df = pd.DataFrame(final_list,columns=['Title','Review','Rating','Likes','Experience','Location'])\n",
    "print(df)\n",
    "df.to_csv('quoted.csv')\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
